[step: 100] loss: 8.8546 accuracy: 0.0112
[step: 200] loss: 8.5618 accuracy: 0.0152
==== epoch: 0, lr:0.001 ====
[step: 203] loss: 8.5508 accuracy: 0.0157
[Valid]: [step: 203] loss: 4.9118 accuracy: 0.0244
[step: 303] loss: 7.6987 accuracy: 0.0481
[step: 403] loss: 7.5622 accuracy: 0.0548
==== epoch: 1, lr:0.001 ====
[step: 406] loss: 7.5625 accuracy: 0.0548
[Valid]: [step: 406] loss: 4.6402 accuracy: 0.0638
[step: 506] loss: 6.8545 accuracy: 0.0956
[step: 606] loss: 6.7450 accuracy: 0.1039
==== epoch: 2, lr:0.001 ====
[step: 609] loss: 6.7427 accuracy: 0.1041
[Valid]: [step: 609] loss: 3.8555 accuracy: 0.1231
[step: 709] loss: 6.1067 accuracy: 0.1709
[step: 809] loss: 5.9811 accuracy: 0.1830
==== epoch: 3, lr:0.001 ====
[step: 812] loss: 5.9832 accuracy: 0.1827
[Valid]: [step: 812] loss: 3.4380 accuracy: 0.1800
[step: 912] loss: 5.3371 accuracy: 0.2628
[step: 1012] loss: 5.2703 accuracy: 0.2655
==== epoch: 4, lr:0.001 ====
[step: 1015] loss: 5.2672 accuracy: 0.2665
[Valid]: [step: 1015] loss: 3.3094 accuracy: 0.2175
[step: 1115] loss: 4.6476 accuracy: 0.3588
[step: 1215] loss: 4.6183 accuracy: 0.3598
==== epoch: 5, lr:0.001 ====
[step: 1218] loss: 4.6125 accuracy: 0.3613
[Valid]: [step: 1218] loss: 2.8495 accuracy: 0.2819
[step: 1318] loss: 4.0869 accuracy: 0.4447
[step: 1418] loss: 4.0874 accuracy: 0.4472
==== epoch: 6, lr:0.001 ====
[step: 1421] loss: 4.0872 accuracy: 0.4474
[Valid]: [step: 1421] loss: 2.2920 accuracy: 0.4125
[step: 1521] loss: 3.6099 accuracy: 0.5288
[step: 1621] loss: 3.6092 accuracy: 0.5233
==== epoch: 7, lr:0.001 ====
[step: 1624] loss: 3.6097 accuracy: 0.5232
[Valid]: [step: 1624] loss: 2.5010 accuracy: 0.3588
[step: 1724] loss: 3.1574 accuracy: 0.6028
[step: 1824] loss: 3.1616 accuracy: 0.5972
==== epoch: 8, lr:0.001 ====
[step: 1827] loss: 3.1604 accuracy: 0.5967
[Valid]: [step: 1827] loss: 2.0373 accuracy: 0.4537
[step: 1927] loss: 2.8405 accuracy: 0.6503
[step: 2027] loss: 2.8650 accuracy: 0.6456
==== epoch: 9, lr:0.001 ====
[step: 2030] loss: 2.8648 accuracy: 0.6467
[Valid]: [step: 2030] loss: 1.6341 accuracy: 0.5450
[step: 2130] loss: 2.5082 accuracy: 0.7172
[step: 2230] loss: 2.5633 accuracy: 0.7042
==== epoch: 10, lr:0.001 ====
[step: 2233] loss: 2.5675 accuracy: 0.7026
[Valid]: [step: 2233] loss: 1.6590 accuracy: 0.5531
[step: 2333] loss: 2.2683 accuracy: 0.7528
[step: 2433] loss: 2.3016 accuracy: 0.7459
==== epoch: 11, lr:0.001 ====
[step: 2436] loss: 2.3009 accuracy: 0.7457
[Valid]: [step: 2436] loss: 1.6625 accuracy: 0.5444
[step: 2536] loss: 2.0545 accuracy: 0.7850
[step: 2636] loss: 2.0849 accuracy: 0.7842
==== epoch: 12, lr:0.001 ====
[step: 2639] loss: 2.0865 accuracy: 0.7843
[Valid]: [step: 2639] loss: 1.6274 accuracy: 0.5625
[step: 2739] loss: 1.8593 accuracy: 0.8147
[step: 2839] loss: 1.9016 accuracy: 0.8070
==== epoch: 13, lr:0.001 ====
[step: 2842] loss: 1.9023 accuracy: 0.8071
[Valid]: [step: 2842] loss: 1.7707 accuracy: 0.5400
[step: 2942] loss: 1.6747 accuracy: 0.8422
[step: 3042] loss: 1.7063 accuracy: 0.8423
==== epoch: 14, lr:0.001 ====
[step: 3045] loss: 1.7103 accuracy: 0.8421
[Valid]: [step: 3045] loss: 1.4609 accuracy: 0.6138
[step: 3145] loss: 1.5287 accuracy: 0.8744
[step: 3245] loss: 1.5730 accuracy: 0.8589
==== epoch: 15, lr:0.001 ====
[step: 3248] loss: 1.5770 accuracy: 0.8590
[Valid]: [step: 3248] loss: 1.2825 accuracy: 0.6506
[step: 3348] loss: 1.4195 accuracy: 0.8881
[step: 3448] loss: 1.4670 accuracy: 0.8788
==== epoch: 16, lr:0.001 ====
[step: 3451] loss: 1.4646 accuracy: 0.8787
[Valid]: [step: 3451] loss: 1.2480 accuracy: 0.6706
[step: 3551] loss: 1.3310 accuracy: 0.8906
[step: 3651] loss: 1.3404 accuracy: 0.8934
==== epoch: 17, lr:0.001 ====
[step: 3654] loss: 1.3461 accuracy: 0.8929
[Valid]: [step: 3654] loss: 1.3283 accuracy: 0.6675
[step: 3754] loss: 1.2428 accuracy: 0.9025
[step: 3854] loss: 1.2820 accuracy: 0.8970
==== epoch: 18, lr:0.001 ====
[step: 3857] loss: 1.2849 accuracy: 0.8966
[Valid]: [step: 3857] loss: 1.6392 accuracy: 0.5981
[step: 3957] loss: 1.1757 accuracy: 0.9113
[step: 4057] loss: 1.1749 accuracy: 0.9098
==== epoch: 19, lr:0.001 ====
[step: 4060] loss: 1.1770 accuracy: 0.9104
[Valid]: [step: 4060] loss: 1.2292 accuracy: 0.6856
[step: 4160] loss: 1.0640 accuracy: 0.9297
[step: 4260] loss: 1.1029 accuracy: 0.9231
==== epoch: 20, lr:0.001 ====
[step: 4263] loss: 1.1019 accuracy: 0.9230
[Valid]: [step: 4263] loss: 1.1606 accuracy: 0.6994
[step: 4363] loss: 0.9880 accuracy: 0.9394
[step: 4463] loss: 1.0178 accuracy: 0.9359
==== epoch: 21, lr:0.001 ====
[step: 4466] loss: 1.0208 accuracy: 0.9355
[Valid]: [step: 4466] loss: 1.1942 accuracy: 0.7000
[step: 4566] loss: 0.9603 accuracy: 0.9350
[step: 4666] loss: 0.9949 accuracy: 0.9300
==== epoch: 22, lr:0.001 ====
[step: 4669] loss: 0.9949 accuracy: 0.9303
[Valid]: [step: 4669] loss: 1.3211 accuracy: 0.6875
[step: 4769] loss: 0.9256 accuracy: 0.9403
[step: 4869] loss: 0.9357 accuracy: 0.9330
==== epoch: 23, lr:0.001 ====
[step: 4872] loss: 0.9369 accuracy: 0.9333
[Valid]: [step: 4872] loss: 1.1885 accuracy: 0.7013
[step: 4972] loss: 0.8511 accuracy: 0.9475
[step: 5072] loss: 0.8827 accuracy: 0.9441
==== epoch: 24, lr:0.001 ====
[step: 5075] loss: 0.8824 accuracy: 0.9438
[Valid]: [step: 5075] loss: 1.1850 accuracy: 0.7087
[step: 5175] loss: 0.8037 accuracy: 0.9522
[step: 5275] loss: 0.8277 accuracy: 0.9478
==== epoch: 25, lr:0.001 ====
[step: 5278] loss: 0.8308 accuracy: 0.9477
[Valid]: [step: 5278] loss: 1.1977 accuracy: 0.6956
[step: 5378] loss: 0.7877 accuracy: 0.9463
[step: 5478] loss: 0.8365 accuracy: 0.9384
==== epoch: 26, lr:0.001 ====
[step: 5481] loss: 0.8375 accuracy: 0.9386
[Valid]: [step: 5481] loss: 1.0459 accuracy: 0.7381
[step: 5581] loss: 0.7720 accuracy: 0.9509
[step: 5681] loss: 0.7969 accuracy: 0.9470
==== epoch: 27, lr:0.001 ====
[step: 5684] loss: 0.7977 accuracy: 0.9467
[Valid]: [step: 5684] loss: 1.2099 accuracy: 0.7169
[step: 5784] loss: 0.7223 accuracy: 0.9578
[step: 5884] loss: 0.7628 accuracy: 0.9486
==== epoch: 28, lr:0.001 ====
[step: 5887] loss: 0.7628 accuracy: 0.9489
[Valid]: [step: 5887] loss: 1.1759 accuracy: 0.7150
[step: 5987] loss: 0.6909 accuracy: 0.9603
[step: 6087] loss: 0.7099 accuracy: 0.9580
==== epoch: 29, lr:0.001 ====
[step: 6090] loss: 0.7096 accuracy: 0.9586
[Valid]: [step: 6090] loss: 1.1374 accuracy: 0.7319
[step: 6190] loss: 0.6641 accuracy: 0.9634
[step: 6290] loss: 0.7071 accuracy: 0.9561
==== epoch: 30, lr:0.001 ====
[step: 6293] loss: 0.7073 accuracy: 0.9560
[Valid]: [step: 6293] loss: 1.2113 accuracy: 0.7106
[step: 6393] loss: 0.6362 accuracy: 0.9625
[step: 6493] loss: 0.6603 accuracy: 0.9594
==== epoch: 31, lr:0.001 ====
[step: 6496] loss: 0.6610 accuracy: 0.9591
[Valid]: [step: 6496] loss: 1.1892 accuracy: 0.7231
[step: 6596] loss: 0.6079 accuracy: 0.9625
[step: 6696] loss: 0.6378 accuracy: 0.9627
==== epoch: 32, lr:0.001 ====
[step: 6699] loss: 0.6420 accuracy: 0.9624
[Valid]: [step: 6699] loss: 1.1634 accuracy: 0.7350
[step: 6799] loss: 0.5806 accuracy: 0.9709
[step: 6899] loss: 0.6121 accuracy: 0.9692
==== epoch: 33, lr:0.001 ====
[step: 6902] loss: 0.6139 accuracy: 0.9689
[Valid]: [step: 6902] loss: 1.2032 accuracy: 0.7406
[step: 7002] loss: 0.5624 accuracy: 0.9666
[step: 7102] loss: 0.5892 accuracy: 0.9647
==== epoch: 34, lr:0.001 ====
[step: 7105] loss: 0.5906 accuracy: 0.9646
[Valid]: [step: 7105] loss: 1.3337 accuracy: 0.6950
[step: 7205] loss: 0.6002 accuracy: 0.9609
[step: 7305] loss: 0.6262 accuracy: 0.9567
==== epoch: 35, lr:0.001 ====
[step: 7308] loss: 0.6264 accuracy: 0.9563
[Valid]: [step: 7308] loss: 1.2188 accuracy: 0.7175
[step: 7408] loss: 0.5549 accuracy: 0.9597
[step: 7508] loss: 0.5970 accuracy: 0.9591
==== epoch: 36, lr:0.001 ====
[step: 7511] loss: 0.5986 accuracy: 0.9587
[Valid]: [step: 7511] loss: 1.3621 accuracy: 0.7063
[step: 7611] loss: 0.5220 accuracy: 0.9678
[step: 7711] loss: 0.5615 accuracy: 0.9661
==== epoch: 37, lr:0.001 ====
[step: 7714] loss: 0.5645 accuracy: 0.9660
[Valid]: [step: 7714] loss: 1.3188 accuracy: 0.7250
[step: 7814] loss: 0.5076 accuracy: 0.9747
[step: 7914] loss: 0.5258 accuracy: 0.9719
==== epoch: 38, lr:0.001 ====
[step: 7917] loss: 0.5250 accuracy: 0.9720
[Valid]: [step: 7917] loss: 1.1273 accuracy: 0.7500
[step: 8017] loss: 0.4852 accuracy: 0.9694
[step: 8117] loss: 0.5162 accuracy: 0.9675
==== epoch: 39, lr:0.001 ====
[step: 8120] loss: 0.5185 accuracy: 0.9674
[Valid]: [step: 8120] loss: 1.3251 accuracy: 0.7256
[step: 8220] loss: 0.4938 accuracy: 0.9681
[step: 8320] loss: 0.4993 accuracy: 0.9706
==== epoch: 40, lr:0.001 ====
[step: 8323] loss: 0.4986 accuracy: 0.9708
[Valid]: [step: 8323] loss: 1.2776 accuracy: 0.7275
[step: 8423] loss: 0.4646 accuracy: 0.9709
[step: 8523] loss: 0.4909 accuracy: 0.9684
==== epoch: 41, lr:0.001 ====
[step: 8526] loss: 0.4925 accuracy: 0.9683
[Valid]: [step: 8526] loss: 1.1256 accuracy: 0.7438
[step: 8626] loss: 0.4347 accuracy: 0.9753
[step: 8726] loss: 0.4788 accuracy: 0.9711
==== epoch: 42, lr:0.001 ====
[step: 8729] loss: 0.4783 accuracy: 0.9711
[Valid]: [step: 8729] loss: 1.3339 accuracy: 0.7144
[step: 8829] loss: 0.4447 accuracy: 0.9800
[step: 8929] loss: 0.4633 accuracy: 0.9770
==== epoch: 43, lr:0.001 ====
[step: 8932] loss: 0.4628 accuracy: 0.9772
[Valid]: [step: 8932] loss: 1.2283 accuracy: 0.7356
[step: 9032] loss: 0.4228 accuracy: 0.9741
[step: 9132] loss: 0.4487 accuracy: 0.9698
==== epoch: 44, lr:0.001 ====
[step: 9135] loss: 0.4482 accuracy: 0.9701
[Valid]: [step: 9135] loss: 1.2653 accuracy: 0.7375
[step: 9235] loss: 0.4315 accuracy: 0.9731
[step: 9335] loss: 0.4609 accuracy: 0.9698
==== epoch: 45, lr:0.001 ====
[step: 9338] loss: 0.4612 accuracy: 0.9701
[Valid]: [step: 9338] loss: 1.2892 accuracy: 0.7219
[step: 9438] loss: 0.4367 accuracy: 0.9694
[step: 9538] loss: 0.4502 accuracy: 0.9697
==== epoch: 46, lr:0.001 ====
[step: 9541] loss: 0.4518 accuracy: 0.9695
[Valid]: [step: 9541] loss: 1.1762 accuracy: 0.7438
[step: 9641] loss: 0.4352 accuracy: 0.9741
[step: 9741] loss: 0.4430 accuracy: 0.9692
==== epoch: 47, lr:0.001 ====
[step: 9744] loss: 0.4444 accuracy: 0.9691
[Valid]: [step: 9744] loss: 1.2355 accuracy: 0.7450
[step: 9844] loss: 0.3642 accuracy: 0.9838
[step: 9944] loss: 0.3922 accuracy: 0.9808
==== epoch: 48, lr:0.001 ====
[step: 9947] loss: 0.3919 accuracy: 0.9808
[Valid]: [step: 9947] loss: 1.2352 accuracy: 0.7444
[step: 10047] loss: 0.4021 accuracy: 0.9712
[step: 10147] loss: 0.4202 accuracy: 0.9698
==== epoch: 49, lr:0.001 ====
[step: 10150] loss: 0.4191 accuracy: 0.9701
[Valid]: [step: 10150] loss: 1.3295 accuracy: 0.7275
[step: 10250] loss: 0.3137 accuracy: 0.9866
[step: 10350] loss: 0.2845 accuracy: 0.9880
==== epoch: 50, lr:0.0001 ====
[step: 10353] loss: 0.2840 accuracy: 0.9881
[Valid]: [step: 10353] loss: 1.0200 accuracy: 0.7844
[step: 10453] loss: 0.2165 accuracy: 0.9953
[step: 10553] loss: 0.2183 accuracy: 0.9945
==== epoch: 51, lr:0.0001 ====
[step: 10556] loss: 0.2176 accuracy: 0.9943
[Valid]: [step: 10556] loss: 0.9927 accuracy: 0.7969
[step: 10656] loss: 0.1857 accuracy: 0.9953
[step: 10756] loss: 0.1940 accuracy: 0.9953
==== epoch: 52, lr:0.0001 ====
[step: 10759] loss: 0.1936 accuracy: 0.9952
[Valid]: [step: 10759] loss: 0.9642 accuracy: 0.7975
[step: 10859] loss: 0.1836 accuracy: 0.9959
[step: 10959] loss: 0.1838 accuracy: 0.9953
==== epoch: 53, lr:0.0001 ====
[step: 10962] loss: 0.1833 accuracy: 0.9954
[Valid]: [step: 10962] loss: 0.9561 accuracy: 0.8006
[step: 11062] loss: 0.1759 accuracy: 0.9972
[step: 11162] loss: 0.1789 accuracy: 0.9955
==== epoch: 54, lr:0.0001 ====
[step: 11165] loss: 0.1791 accuracy: 0.9955
[Valid]: [step: 11165] loss: 0.9456 accuracy: 0.7981
[step: 11265] loss: 0.1712 accuracy: 0.9956
[step: 11365] loss: 0.1641 accuracy: 0.9964
==== epoch: 55, lr:0.0001 ====
[step: 11368] loss: 0.1644 accuracy: 0.9965
[Valid]: [step: 11368] loss: 0.9534 accuracy: 0.7975
[step: 11468] loss: 0.1711 accuracy: 0.9950
[step: 11568] loss: 0.1735 accuracy: 0.9947
==== epoch: 56, lr:0.0001 ====
[step: 11571] loss: 0.1732 accuracy: 0.9948
[Valid]: [step: 11571] loss: 0.9328 accuracy: 0.8019
[step: 11671] loss: 0.1645 accuracy: 0.9966
[step: 11771] loss: 0.1652 accuracy: 0.9956
==== epoch: 57, lr:0.0001 ====
[step: 11774] loss: 0.1647 accuracy: 0.9957
[Valid]: [step: 11774] loss: 0.9383 accuracy: 0.7969
[step: 11874] loss: 0.1549 accuracy: 0.9962
[step: 11974] loss: 0.1548 accuracy: 0.9967
==== epoch: 58, lr:0.0001 ====
[step: 11977] loss: 0.1539 accuracy: 0.9968
[Valid]: [step: 11977] loss: 0.9216 accuracy: 0.8031
[step: 12077] loss: 0.1509 accuracy: 0.9981
[step: 12177] loss: 0.1531 accuracy: 0.9969
==== epoch: 59, lr:0.0001 ====
[step: 12180] loss: 0.1529 accuracy: 0.9968
[Valid]: [step: 12180] loss: 0.9137 accuracy: 0.8113
[step: 12280] loss: 0.1478 accuracy: 0.9981
[step: 12380] loss: 0.1488 accuracy: 0.9966
==== epoch: 60, lr:0.0001 ====
[step: 12383] loss: 0.1486 accuracy: 0.9965
[Valid]: [step: 12383] loss: 0.9125 accuracy: 0.8063
[step: 12483] loss: 0.1434 accuracy: 0.9959
[step: 12583] loss: 0.1459 accuracy: 0.9961
==== epoch: 61, lr:0.0001 ====
[step: 12586] loss: 0.1455 accuracy: 0.9962
[Valid]: [step: 12586] loss: 0.9331 accuracy: 0.8087
[step: 12686] loss: 0.1362 accuracy: 0.9966
[step: 12786] loss: 0.1409 accuracy: 0.9969
==== epoch: 62, lr:0.0001 ====
[step: 12789] loss: 0.1405 accuracy: 0.9969
[Valid]: [step: 12789] loss: 0.9221 accuracy: 0.8031
[step: 12889] loss: 0.1327 accuracy: 0.9956
[step: 12989] loss: 0.1406 accuracy: 0.9966
==== epoch: 63, lr:0.0001 ====
[step: 12992] loss: 0.1403 accuracy: 0.9966
[Valid]: [step: 12992] loss: 0.9153 accuracy: 0.8037
[step: 13092] loss: 0.1357 accuracy: 0.9978
[step: 13192] loss: 0.1394 accuracy: 0.9962
==== epoch: 64, lr:0.0001 ====
[step: 13195] loss: 0.1390 accuracy: 0.9963
[Valid]: [step: 13195] loss: 0.9360 accuracy: 0.8069
[step: 13295] loss: 0.1422 accuracy: 0.9972
[step: 13395] loss: 0.1374 accuracy: 0.9975
==== epoch: 65, lr:0.0001 ====
[step: 13398] loss: 0.1366 accuracy: 0.9975
[Valid]: [step: 13398] loss: 0.9444 accuracy: 0.8050
[step: 13498] loss: 0.1391 accuracy: 0.9969
[step: 13598] loss: 0.1330 accuracy: 0.9969
==== epoch: 66, lr:0.0001 ====
[step: 13601] loss: 0.1335 accuracy: 0.9969
[Valid]: [step: 13601] loss: 0.9089 accuracy: 0.8075
[step: 13701] loss: 0.1413 accuracy: 0.9969
[step: 13801] loss: 0.1373 accuracy: 0.9969
==== epoch: 67, lr:0.0001 ====
[step: 13804] loss: 0.1376 accuracy: 0.9969
[Valid]: [step: 13804] loss: 0.9051 accuracy: 0.8106
[step: 13904] loss: 0.1138 accuracy: 0.9984
[step: 14004] loss: 0.1230 accuracy: 0.9975
==== epoch: 68, lr:0.0001 ====
[step: 14007] loss: 0.1230 accuracy: 0.9975
[Valid]: [step: 14007] loss: 0.9269 accuracy: 0.8119
[step: 14107] loss: 0.1220 accuracy: 0.9984
[step: 14207] loss: 0.1258 accuracy: 0.9972
==== epoch: 69, lr:0.0001 ====
[step: 14210] loss: 0.1262 accuracy: 0.9972
[Valid]: [step: 14210] loss: 0.9300 accuracy: 0.8119
[step: 14310] loss: 0.1318 accuracy: 0.9953
[step: 14410] loss: 0.1251 accuracy: 0.9969
==== epoch: 70, lr:0.0001 ====
[step: 14413] loss: 0.1241 accuracy: 0.9969
[Valid]: [step: 14413] loss: 0.9255 accuracy: 0.8119
[step: 14513] loss: 0.1168 accuracy: 0.9994
[step: 14613] loss: 0.1216 accuracy: 0.9978
==== epoch: 71, lr:0.0001 ====
[step: 14616] loss: 0.1209 accuracy: 0.9978
[Valid]: [step: 14616] loss: 0.9299 accuracy: 0.8113
[step: 14716] loss: 0.1112 accuracy: 0.9984
[step: 14816] loss: 0.1171 accuracy: 0.9977
==== epoch: 72, lr:0.0001 ====
[step: 14819] loss: 0.1175 accuracy: 0.9977
[Valid]: [step: 14819] loss: 0.9389 accuracy: 0.8075
[step: 14919] loss: 0.1153 accuracy: 0.9972
[step: 15019] loss: 0.1182 accuracy: 0.9967
==== epoch: 73, lr:0.0001 ====
[step: 15022] loss: 0.1179 accuracy: 0.9968
[Valid]: [step: 15022] loss: 0.9480 accuracy: 0.8013
[step: 15122] loss: 0.1172 accuracy: 0.9969
[step: 15222] loss: 0.1196 accuracy: 0.9969
==== epoch: 74, lr:0.0001 ====
[step: 15225] loss: 0.1196 accuracy: 0.9969
[Valid]: [step: 15225] loss: 0.9319 accuracy: 0.8069
[step: 15325] loss: 0.1153 accuracy: 0.9981
[step: 15425] loss: 0.1175 accuracy: 0.9978
==== epoch: 75, lr:0.0001 ====
[step: 15428] loss: 0.1175 accuracy: 0.9978
[Valid]: [step: 15428] loss: 0.9202 accuracy: 0.8137
[step: 15528] loss: 0.1269 accuracy: 0.9969
[step: 15628] loss: 0.1197 accuracy: 0.9967
==== epoch: 76, lr:0.0001 ====
[step: 15631] loss: 0.1193 accuracy: 0.9968
[Valid]: [step: 15631] loss: 0.9231 accuracy: 0.8119
[step: 15731] loss: 0.1129 accuracy: 0.9969
[step: 15831] loss: 0.1164 accuracy: 0.9970
==== epoch: 77, lr:0.0001 ====
[step: 15834] loss: 0.1163 accuracy: 0.9971
[Valid]: [step: 15834] loss: 0.9294 accuracy: 0.8169
[step: 15934] loss: 0.1130 accuracy: 0.9969
[step: 16034] loss: 0.1178 accuracy: 0.9967
==== epoch: 78, lr:0.0001 ====
[step: 16037] loss: 0.1182 accuracy: 0.9968
[Valid]: [step: 16037] loss: 0.9394 accuracy: 0.8063
[step: 16137] loss: 0.1043 accuracy: 0.9988
[step: 16237] loss: 0.1083 accuracy: 0.9973
==== epoch: 79, lr:0.0001 ====
[step: 16240] loss: 0.1084 accuracy: 0.9974
[Valid]: [step: 16240] loss: 0.9350 accuracy: 0.8156
[step: 16340] loss: 0.1096 accuracy: 0.9981
[step: 16440] loss: 0.1106 accuracy: 0.9978
==== epoch: 80, lr:0.0001 ====
[step: 16443] loss: 0.1099 accuracy: 0.9978
[Valid]: [step: 16443] loss: 0.9273 accuracy: 0.8156
[step: 16543] loss: 0.0971 accuracy: 0.9994
[step: 16643] loss: 0.1065 accuracy: 0.9975
==== epoch: 81, lr:0.0001 ====
[step: 16646] loss: 0.1066 accuracy: 0.9974
[Valid]: [step: 16646] loss: 0.9615 accuracy: 0.8069
[step: 16746] loss: 0.0977 accuracy: 0.9981
[step: 16846] loss: 0.1031 accuracy: 0.9978
==== epoch: 82, lr:0.0001 ====
[step: 16849] loss: 0.1030 accuracy: 0.9978
[Valid]: [step: 16849] loss: 0.9588 accuracy: 0.8037
[step: 16949] loss: 0.1022 accuracy: 0.9978
[step: 17049] loss: 0.1049 accuracy: 0.9980
==== epoch: 83, lr:0.0001 ====
[step: 17052] loss: 0.1047 accuracy: 0.9980
[Valid]: [step: 17052] loss: 0.9523 accuracy: 0.8113
[step: 17152] loss: 0.1130 accuracy: 0.9975
[step: 17252] loss: 0.1097 accuracy: 0.9973
==== epoch: 84, lr:0.0001 ====
[step: 17255] loss: 0.1094 accuracy: 0.9974
[Valid]: [step: 17255] loss: 0.9488 accuracy: 0.8137
[step: 17355] loss: 0.1046 accuracy: 0.9988
[step: 17455] loss: 0.1088 accuracy: 0.9978
==== epoch: 85, lr:0.0001 ====
[step: 17458] loss: 0.1084 accuracy: 0.9978
[Valid]: [step: 17458] loss: 0.9668 accuracy: 0.8056
[step: 17558] loss: 0.1060 accuracy: 0.9969
[step: 17658] loss: 0.1084 accuracy: 0.9972
==== epoch: 86, lr:0.0001 ====
[step: 17661] loss: 0.1082 accuracy: 0.9972
[Valid]: [step: 17661] loss: 0.9632 accuracy: 0.8081
[step: 17761] loss: 0.0985 accuracy: 0.9981
[step: 17861] loss: 0.1055 accuracy: 0.9970
==== epoch: 87, lr:0.0001 ====
[step: 17864] loss: 0.1053 accuracy: 0.9971
[Valid]: [step: 17864] loss: 0.9511 accuracy: 0.8069
[step: 17964] loss: 0.1003 accuracy: 0.9981
[step: 18064] loss: 0.1007 accuracy: 0.9977
==== epoch: 88, lr:0.0001 ====
[step: 18067] loss: 0.1007 accuracy: 0.9975
[Valid]: [step: 18067] loss: 0.9417 accuracy: 0.8100
[step: 18167] loss: 0.1042 accuracy: 0.9975
[step: 18267] loss: 0.1026 accuracy: 0.9973
==== epoch: 89, lr:0.0001 ====
[step: 18270] loss: 0.1022 accuracy: 0.9974
[Valid]: [step: 18270] loss: 0.9685 accuracy: 0.8144
[step: 18370] loss: 0.0956 accuracy: 0.9975
[step: 18470] loss: 0.1011 accuracy: 0.9973
==== epoch: 90, lr:0.0001 ====
[step: 18473] loss: 0.1011 accuracy: 0.9974
[Valid]: [step: 18473] loss: 0.9520 accuracy: 0.8137
[step: 18573] loss: 0.0969 accuracy: 0.9988
[step: 18673] loss: 0.0979 accuracy: 0.9973
==== epoch: 91, lr:0.0001 ====
[step: 18676] loss: 0.0982 accuracy: 0.9972
[Valid]: [step: 18676] loss: 0.9860 accuracy: 0.8087
[step: 18776] loss: 0.0877 accuracy: 0.9988
[step: 18876] loss: 0.0938 accuracy: 0.9977
==== epoch: 92, lr:0.0001 ====
[step: 18879] loss: 0.0940 accuracy: 0.9977
[Valid]: [step: 18879] loss: 0.9740 accuracy: 0.8056
[step: 18979] loss: 0.0920 accuracy: 0.9988
[step: 19079] loss: 0.0964 accuracy: 0.9972
==== epoch: 93, lr:0.0001 ====
[step: 19082] loss: 0.0965 accuracy: 0.9972
[Valid]: [step: 19082] loss: 0.9715 accuracy: 0.8113
[step: 19182] loss: 0.0956 accuracy: 0.9966
[step: 19282] loss: 0.0914 accuracy: 0.9978
==== epoch: 94, lr:0.0001 ====
[step: 19285] loss: 0.0921 accuracy: 0.9978
[Valid]: [step: 19285] loss: 0.9758 accuracy: 0.8125
[step: 19385] loss: 0.0980 accuracy: 0.9978
[step: 19485] loss: 0.0963 accuracy: 0.9980
==== epoch: 95, lr:0.0001 ====
[step: 19488] loss: 0.0966 accuracy: 0.9980
[Valid]: [step: 19488] loss: 0.9616 accuracy: 0.8100
[step: 19588] loss: 0.0899 accuracy: 0.9991
[step: 19688] loss: 0.0936 accuracy: 0.9978
==== epoch: 96, lr:0.0001 ====
[step: 19691] loss: 0.0937 accuracy: 0.9978
[Valid]: [step: 19691] loss: 0.9890 accuracy: 0.8019
[step: 19791] loss: 0.0968 accuracy: 0.9981
[step: 19891] loss: 0.0972 accuracy: 0.9972
==== epoch: 97, lr:0.0001 ====
[step: 19894] loss: 0.0969 accuracy: 0.9972
[Valid]: [step: 19894] loss: 0.9636 accuracy: 0.8081
[step: 19994] loss: 0.0943 accuracy: 0.9966
[step: 20094] loss: 0.0952 accuracy: 0.9973
==== epoch: 98, lr:0.0001 ====
[step: 20097] loss: 0.0948 accuracy: 0.9974
[Valid]: [step: 20097] loss: 0.9778 accuracy: 0.8119
[step: 20197] loss: 0.0877 accuracy: 0.9975
[step: 20297] loss: 0.0895 accuracy: 0.9975
==== epoch: 99, lr:0.0001 ====
[step: 20300] loss: 0.0894 accuracy: 0.9975
[Valid]: [step: 20300] loss: 0.9729 accuracy: 0.8113
[step: 20400] loss: 0.0928 accuracy: 0.9972
[step: 20500] loss: 0.0933 accuracy: 0.9975
==== epoch: 100, lr:0.0001 ====
[step: 20503] loss: 0.0934 accuracy: 0.9975
[Valid]: [step: 20503] loss: 0.9841 accuracy: 0.8144
[step: 20603] loss: 0.0853 accuracy: 0.9972
[step: 20703] loss: 0.0886 accuracy: 0.9972
==== epoch: 101, lr:0.0001 ====
[step: 20706] loss: 0.0884 accuracy: 0.9971
[Valid]: [step: 20706] loss: 0.9666 accuracy: 0.8169
[step: 20806] loss: 0.0951 accuracy: 0.9981
[step: 20906] loss: 0.0946 accuracy: 0.9977
==== epoch: 102, lr:0.0001 ====
[step: 20909] loss: 0.0942 accuracy: 0.9977
[Valid]: [step: 20909] loss: 0.9695 accuracy: 0.8113
[step: 21009] loss: 0.0850 accuracy: 0.9984
[step: 21109] loss: 0.0855 accuracy: 0.9981
==== epoch: 103, lr:0.0001 ====
[step: 21112] loss: 0.0857 accuracy: 0.9982
[Valid]: [step: 21112] loss: 0.9858 accuracy: 0.8081
[step: 21212] loss: 0.0840 accuracy: 0.9978
[step: 21312] loss: 0.0856 accuracy: 0.9977
==== epoch: 104, lr:0.0001 ====
[step: 21315] loss: 0.0861 accuracy: 0.9975
[Valid]: [step: 21315] loss: 0.9800 accuracy: 0.8094
[step: 21415] loss: 0.0860 accuracy: 0.9984
[step: 21515] loss: 0.0875 accuracy: 0.9981
==== epoch: 105, lr:0.0001 ====
[step: 21518] loss: 0.0872 accuracy: 0.9982
[Valid]: [step: 21518] loss: 0.9900 accuracy: 0.8156
[step: 21618] loss: 0.0751 accuracy: 0.9988
[step: 21718] loss: 0.0818 accuracy: 0.9978
==== epoch: 106, lr:0.0001 ====
[step: 21721] loss: 0.0819 accuracy: 0.9978
[Valid]: [step: 21721] loss: 0.9670 accuracy: 0.8200
[step: 21821] loss: 0.0774 accuracy: 0.9984
[step: 21921] loss: 0.0818 accuracy: 0.9978
==== epoch: 107, lr:0.0001 ====
[step: 21924] loss: 0.0816 accuracy: 0.9978
[Valid]: [step: 21924] loss: 0.9645 accuracy: 0.8175
[step: 22024] loss: 0.0854 accuracy: 0.9981
[step: 22124] loss: 0.0867 accuracy: 0.9975
==== epoch: 108, lr:0.0001 ====
[step: 22127] loss: 0.0869 accuracy: 0.9975
[Valid]: [step: 22127] loss: 0.9799 accuracy: 0.8137
[step: 22227] loss: 0.0770 accuracy: 0.9962
[step: 22327] loss: 0.0832 accuracy: 0.9969
==== epoch: 109, lr:0.0001 ====
[step: 22330] loss: 0.0834 accuracy: 0.9969
[Valid]: [step: 22330] loss: 0.9948 accuracy: 0.8113
[step: 22430] loss: 0.0786 accuracy: 0.9981
[step: 22530] loss: 0.0819 accuracy: 0.9977
==== epoch: 110, lr:0.0001 ====
[step: 22533] loss: 0.0826 accuracy: 0.9977
[Valid]: [step: 22533] loss: 0.9812 accuracy: 0.8113
[step: 22633] loss: 0.0758 accuracy: 0.9978
[step: 22733] loss: 0.0776 accuracy: 0.9981
==== epoch: 111, lr:0.0001 ====
[step: 22736] loss: 0.0775 accuracy: 0.9982
[Valid]: [step: 22736] loss: 0.9778 accuracy: 0.8025
[step: 22836] loss: 0.0786 accuracy: 0.9991
[step: 22936] loss: 0.0826 accuracy: 0.9981
==== epoch: 112, lr:0.0001 ====
[step: 22939] loss: 0.0831 accuracy: 0.9982
[Valid]: [step: 22939] loss: 0.9776 accuracy: 0.8100
[step: 23039] loss: 0.0873 accuracy: 0.9978
[step: 23139] loss: 0.0808 accuracy: 0.9984
==== epoch: 113, lr:0.0001 ====
[step: 23142] loss: 0.0805 accuracy: 0.9985
[Valid]: [step: 23142] loss: 0.9624 accuracy: 0.8113
[step: 23242] loss: 0.0784 accuracy: 0.9975
[step: 23342] loss: 0.0796 accuracy: 0.9978
==== epoch: 114, lr:0.0001 ====
[step: 23345] loss: 0.0796 accuracy: 0.9978
[Valid]: [step: 23345] loss: 0.9869 accuracy: 0.8113
[step: 23445] loss: 0.0794 accuracy: 0.9981
[step: 23545] loss: 0.0830 accuracy: 0.9978
==== epoch: 115, lr:0.0001 ====
[step: 23548] loss: 0.0829 accuracy: 0.9978
[Valid]: [step: 23548] loss: 0.9885 accuracy: 0.8119
[step: 23648] loss: 0.0755 accuracy: 0.9988
[step: 23748] loss: 0.0769 accuracy: 0.9984
==== epoch: 116, lr:0.0001 ====
[step: 23751] loss: 0.0770 accuracy: 0.9985
[Valid]: [step: 23751] loss: 0.9791 accuracy: 0.8044
[step: 23851] loss: 0.0723 accuracy: 0.9984
[step: 23951] loss: 0.0765 accuracy: 0.9977
==== epoch: 117, lr:0.0001 ====
[step: 23954] loss: 0.0768 accuracy: 0.9977
[Valid]: [step: 23954] loss: 0.9848 accuracy: 0.8044
[step: 24054] loss: 0.0806 accuracy: 0.9997
[step: 24154] loss: 0.0806 accuracy: 0.9988
==== epoch: 118, lr:0.0001 ====
[step: 24157] loss: 0.0805 accuracy: 0.9988
[Valid]: [step: 24157] loss: 0.9825 accuracy: 0.8044
[step: 24257] loss: 0.0698 accuracy: 0.9994
[step: 24357] loss: 0.0754 accuracy: 0.9981
==== epoch: 119, lr:0.0001 ====
[step: 24360] loss: 0.0763 accuracy: 0.9980
[Valid]: [step: 24360] loss: 0.9925 accuracy: 0.8119
[step: 24460] loss: 0.0746 accuracy: 0.9978
[step: 24560] loss: 0.0748 accuracy: 0.9981
==== epoch: 120, lr:0.0001 ====
[step: 24563] loss: 0.0753 accuracy: 0.9982
[Valid]: [step: 24563] loss: 0.9787 accuracy: 0.8087
[step: 24663] loss: 0.0857 accuracy: 0.9972
[step: 24763] loss: 0.0817 accuracy: 0.9981
==== epoch: 121, lr:0.0001 ====
[step: 24766] loss: 0.0812 accuracy: 0.9982
[Valid]: [step: 24766] loss: 0.9711 accuracy: 0.8119
[step: 24866] loss: 0.0726 accuracy: 0.9994
[step: 24966] loss: 0.0754 accuracy: 0.9983
==== epoch: 122, lr:0.0001 ====
[step: 24969] loss: 0.0757 accuracy: 0.9982
[Valid]: [step: 24969] loss: 0.9770 accuracy: 0.8131
[step: 25069] loss: 0.0742 accuracy: 0.9978
[step: 25169] loss: 0.0757 accuracy: 0.9978
==== epoch: 123, lr:0.0001 ====
[step: 25172] loss: 0.0758 accuracy: 0.9978
[Valid]: [step: 25172] loss: 0.9805 accuracy: 0.8119
[step: 25272] loss: 0.0689 accuracy: 0.9984
[step: 25372] loss: 0.0727 accuracy: 0.9983
==== epoch: 124, lr:0.0001 ====
[step: 25375] loss: 0.0728 accuracy: 0.9982
[Valid]: [step: 25375] loss: 0.9823 accuracy: 0.8050
[step: 25475] loss: 0.0734 accuracy: 0.9975
[step: 25575] loss: 0.0741 accuracy: 0.9975
==== epoch: 125, lr:0.0001 ====
[step: 25578] loss: 0.0736 accuracy: 0.9975
[Valid]: [step: 25578] loss: 0.9754 accuracy: 0.8156
[step: 25678] loss: 0.0751 accuracy: 0.9984
[step: 25778] loss: 0.0782 accuracy: 0.9977
==== epoch: 126, lr:0.0001 ====
[step: 25781] loss: 0.0780 accuracy: 0.9977
[Valid]: [step: 25781] loss: 0.9499 accuracy: 0.8150
[step: 25881] loss: 0.0664 accuracy: 0.9997
[step: 25981] loss: 0.0722 accuracy: 0.9988
==== epoch: 127, lr:0.0001 ====
[step: 25984] loss: 0.0729 accuracy: 0.9986
[Valid]: [step: 25984] loss: 0.9711 accuracy: 0.8050
[step: 26084] loss: 0.0700 accuracy: 0.9975
[step: 26184] loss: 0.0738 accuracy: 0.9973
==== epoch: 128, lr:0.0001 ====
[step: 26187] loss: 0.0738 accuracy: 0.9972
[Valid]: [step: 26187] loss: 0.9743 accuracy: 0.8137
[step: 26287] loss: 0.0738 accuracy: 0.9988
[step: 26387] loss: 0.0762 accuracy: 0.9972
==== epoch: 129, lr:0.0001 ====
[step: 26390] loss: 0.0768 accuracy: 0.9972
[Valid]: [step: 26390] loss: 0.9913 accuracy: 0.8169
[step: 26490] loss: 0.0730 accuracy: 0.9981
[step: 26590] loss: 0.0736 accuracy: 0.9980
==== epoch: 130, lr:0.0001 ====
[step: 26593] loss: 0.0743 accuracy: 0.9978
[Valid]: [step: 26593] loss: 0.9703 accuracy: 0.8131
[step: 26693] loss: 0.0712 accuracy: 0.9984
[step: 26793] loss: 0.0695 accuracy: 0.9981
==== epoch: 131, lr:0.0001 ====
[step: 26796] loss: 0.0693 accuracy: 0.9982
[Valid]: [step: 26796] loss: 0.9567 accuracy: 0.8206
[step: 26896] loss: 0.0708 accuracy: 0.9978
[step: 26996] loss: 0.0754 accuracy: 0.9978
==== epoch: 132, lr:0.0001 ====
[step: 26999] loss: 0.0757 accuracy: 0.9978
[Valid]: [step: 26999] loss: 0.9565 accuracy: 0.8125
[step: 27099] loss: 0.0743 accuracy: 0.9981
[step: 27199] loss: 0.0735 accuracy: 0.9978
==== epoch: 133, lr:0.0001 ====
[step: 27202] loss: 0.0737 accuracy: 0.9977
[Valid]: [step: 27202] loss: 0.9783 accuracy: 0.8125
[step: 27302] loss: 0.0677 accuracy: 0.9984
[step: 27402] loss: 0.0694 accuracy: 0.9978
==== epoch: 134, lr:0.0001 ====
[step: 27405] loss: 0.0692 accuracy: 0.9978
[Valid]: [step: 27405] loss: 0.9902 accuracy: 0.8100
[step: 27505] loss: 0.0707 accuracy: 0.9975
[step: 27605] loss: 0.0721 accuracy: 0.9980
==== epoch: 135, lr:0.0001 ====
[step: 27608] loss: 0.0721 accuracy: 0.9980
[Valid]: [step: 27608] loss: 0.9927 accuracy: 0.8181
[step: 27708] loss: 0.0708 accuracy: 0.9975
[step: 27808] loss: 0.0727 accuracy: 0.9975
==== epoch: 136, lr:0.0001 ====
[step: 27811] loss: 0.0733 accuracy: 0.9974
[Valid]: [step: 27811] loss: 0.9910 accuracy: 0.8119
[step: 27911] loss: 0.0722 accuracy: 0.9984
[step: 28011] loss: 0.0702 accuracy: 0.9984
==== epoch: 137, lr:0.0001 ====
[step: 28014] loss: 0.0700 accuracy: 0.9985
[Valid]: [step: 28014] loss: 0.9785 accuracy: 0.8181
[step: 28114] loss: 0.0657 accuracy: 0.9988
[step: 28214] loss: 0.0689 accuracy: 0.9980
==== epoch: 138, lr:0.0001 ====
[step: 28217] loss: 0.0686 accuracy: 0.9980
[Valid]: [step: 28217] loss: 0.9944 accuracy: 0.8137
[step: 28317] loss: 0.0641 accuracy: 0.9984
[step: 28417] loss: 0.0685 accuracy: 0.9978
==== epoch: 139, lr:0.0001 ====
[step: 28420] loss: 0.0690 accuracy: 0.9977
[Valid]: [step: 28420] loss: 1.0015 accuracy: 0.8094
[step: 28520] loss: 0.0662 accuracy: 0.9978
[step: 28620] loss: 0.0675 accuracy: 0.9980
==== epoch: 140, lr:0.0001 ====
[step: 28623] loss: 0.0675 accuracy: 0.9980
[Valid]: [step: 28623] loss: 1.0129 accuracy: 0.8181
[step: 28723] loss: 0.0642 accuracy: 0.9994
[step: 28823] loss: 0.0677 accuracy: 0.9984
==== epoch: 141, lr:0.0001 ====
[step: 28826] loss: 0.0679 accuracy: 0.9983
[Valid]: [step: 28826] loss: 1.0171 accuracy: 0.8125
[step: 28926] loss: 0.0687 accuracy: 0.9978
[step: 29026] loss: 0.0699 accuracy: 0.9981
==== epoch: 142, lr:0.0001 ====
[step: 29029] loss: 0.0703 accuracy: 0.9980
[Valid]: [step: 29029] loss: 1.0025 accuracy: 0.8100
[step: 29129] loss: 0.0698 accuracy: 0.9978
[step: 29229] loss: 0.0689 accuracy: 0.9981
==== epoch: 143, lr:0.0001 ====
[step: 29232] loss: 0.0687 accuracy: 0.9982
[Valid]: [step: 29232] loss: 0.9722 accuracy: 0.8181
[step: 29332] loss: 0.0649 accuracy: 0.9988
[step: 29432] loss: 0.0652 accuracy: 0.9984
==== epoch: 144, lr:0.0001 ====
[step: 29435] loss: 0.0655 accuracy: 0.9983
[Valid]: [step: 29435] loss: 0.9716 accuracy: 0.8125
[step: 29535] loss: 0.0677 accuracy: 0.9984
[step: 29635] loss: 0.0658 accuracy: 0.9983
==== epoch: 145, lr:0.0001 ====
[step: 29638] loss: 0.0655 accuracy: 0.9983
[Valid]: [step: 29638] loss: 0.9878 accuracy: 0.8150
[step: 29738] loss: 0.0654 accuracy: 0.9984
[step: 29838] loss: 0.0682 accuracy: 0.9978
==== epoch: 146, lr:0.0001 ====
[step: 29841] loss: 0.0682 accuracy: 0.9977
[Valid]: [step: 29841] loss: 1.0050 accuracy: 0.8087
[step: 29941] loss: 0.0656 accuracy: 0.9981
[step: 30041] loss: 0.0668 accuracy: 0.9978
==== epoch: 147, lr:0.0001 ====
[step: 30044] loss: 0.0667 accuracy: 0.9978
[Valid]: [step: 30044] loss: 0.9919 accuracy: 0.8119
[step: 30144] loss: 0.0682 accuracy: 0.9962
[step: 30244] loss: 0.0665 accuracy: 0.9972
==== epoch: 148, lr:0.0001 ====
[step: 30247] loss: 0.0666 accuracy: 0.9972
[Valid]: [step: 30247] loss: 0.9828 accuracy: 0.8144
[step: 30347] loss: 0.0611 accuracy: 0.9981
[step: 30447] loss: 0.0628 accuracy: 0.9983
==== epoch: 149, lr:0.0001 ====
[step: 30450] loss: 0.0626 accuracy: 0.9983
[Valid]: [step: 30450] loss: 0.9957 accuracy: 0.8081
[step: 30550] loss: 0.0591 accuracy: 0.9994
[step: 30650] loss: 0.0631 accuracy: 0.9981
==== epoch: 150, lr:0.0001 ====
[step: 30653] loss: 0.0638 accuracy: 0.9982
[Valid]: [step: 30653] loss: 1.0342 accuracy: 0.8019
[step: 30753] loss: 0.0644 accuracy: 0.9975
[step: 30853] loss: 0.0607 accuracy: 0.9980
==== epoch: 151, lr:0.0001 ====
[step: 30856] loss: 0.0604 accuracy: 0.9978
[Valid]: [step: 30856] loss: 1.0304 accuracy: 0.8069
[step: 30956] loss: 0.0591 accuracy: 0.9988
[step: 31056] loss: 0.0622 accuracy: 0.9980
==== epoch: 152, lr:0.0001 ====
[step: 31059] loss: 0.0620 accuracy: 0.9980
[Valid]: [step: 31059] loss: 0.9907 accuracy: 0.8106
[step: 31159] loss: 0.0609 accuracy: 0.9984
[step: 31259] loss: 0.0654 accuracy: 0.9972
==== epoch: 153, lr:0.0001 ====
[step: 31262] loss: 0.0655 accuracy: 0.9972
[Valid]: [step: 31262] loss: 1.0127 accuracy: 0.8087
[step: 31362] loss: 0.0627 accuracy: 0.9978
[step: 31462] loss: 0.0648 accuracy: 0.9980
==== epoch: 154, lr:0.0001 ====
[step: 31465] loss: 0.0647 accuracy: 0.9980
[Valid]: [step: 31465] loss: 0.9927 accuracy: 0.8119
[step: 31565] loss: 0.0682 accuracy: 0.9975
[step: 31665] loss: 0.0618 accuracy: 0.9983
==== epoch: 155, lr:0.0001 ====
[step: 31668] loss: 0.0620 accuracy: 0.9983
[Valid]: [step: 31668] loss: 0.9991 accuracy: 0.8119
[step: 31768] loss: 0.0583 accuracy: 0.9966
[step: 31868] loss: 0.0605 accuracy: 0.9972
==== epoch: 156, lr:0.0001 ====
[step: 31871] loss: 0.0601 accuracy: 0.9972
[Valid]: [step: 31871] loss: 1.0132 accuracy: 0.8150
[step: 31971] loss: 0.0620 accuracy: 0.9981
[step: 32071] loss: 0.0641 accuracy: 0.9978
==== epoch: 157, lr:0.0001 ====
[step: 32074] loss: 0.0644 accuracy: 0.9978
[Valid]: [step: 32074] loss: 1.0231 accuracy: 0.8069
[step: 32174] loss: 0.0617 accuracy: 0.9988
[step: 32274] loss: 0.0632 accuracy: 0.9984
==== epoch: 158, lr:0.0001 ====
[step: 32277] loss: 0.0644 accuracy: 0.9983
[Valid]: [step: 32277] loss: 1.0340 accuracy: 0.8106
[step: 32377] loss: 0.0548 accuracy: 0.9991
[step: 32477] loss: 0.0575 accuracy: 0.9981
==== epoch: 159, lr:0.0001 ====
[step: 32480] loss: 0.0581 accuracy: 0.9982
[Valid]: [step: 32480] loss: 1.0191 accuracy: 0.8175
[step: 32580] loss: 0.0585 accuracy: 0.9991
[step: 32680] loss: 0.0566 accuracy: 0.9986
==== epoch: 160, lr:0.0001 ====
[step: 32683] loss: 0.0565 accuracy: 0.9986
[Valid]: [step: 32683] loss: 1.0095 accuracy: 0.8131
[step: 32783] loss: 0.0654 accuracy: 0.9975
[step: 32883] loss: 0.0626 accuracy: 0.9983
==== epoch: 161, lr:0.0001 ====
[step: 32886] loss: 0.0628 accuracy: 0.9983
[Valid]: [step: 32886] loss: 1.0114 accuracy: 0.8075
[step: 32986] loss: 0.0572 accuracy: 0.9981
[step: 33086] loss: 0.0601 accuracy: 0.9981
==== epoch: 162, lr:0.0001 ====
[step: 33089] loss: 0.0598 accuracy: 0.9982
[Valid]: [step: 33089] loss: 1.0065 accuracy: 0.8125
[step: 33189] loss: 0.0595 accuracy: 0.9991
[step: 33289] loss: 0.0605 accuracy: 0.9980
==== epoch: 163, lr:0.0001 ====
[step: 33292] loss: 0.0608 accuracy: 0.9980
[Valid]: [step: 33292] loss: 1.0208 accuracy: 0.8063
[step: 33392] loss: 0.0681 accuracy: 0.9947
[step: 33492] loss: 0.0621 accuracy: 0.9969
==== epoch: 164, lr:0.0001 ====
[step: 33495] loss: 0.0618 accuracy: 0.9969
[Valid]: [step: 33495] loss: 1.0353 accuracy: 0.8087
[step: 33595] loss: 0.0572 accuracy: 0.9991
[step: 33695] loss: 0.0593 accuracy: 0.9988
==== epoch: 165, lr:0.0001 ====
[step: 33698] loss: 0.0594 accuracy: 0.9988
[Valid]: [step: 33698] loss: 1.0217 accuracy: 0.8094
[step: 33798] loss: 0.0577 accuracy: 0.9991
[step: 33898] loss: 0.0575 accuracy: 0.9984
==== epoch: 166, lr:0.0001 ====
[step: 33901] loss: 0.0571 accuracy: 0.9985
[Valid]: [step: 33901] loss: 1.0053 accuracy: 0.8063
[step: 34001] loss: 0.0598 accuracy: 0.9988
[step: 34101] loss: 0.0615 accuracy: 0.9978
==== epoch: 167, lr:0.0001 ====
[step: 34104] loss: 0.0614 accuracy: 0.9978
[Valid]: [step: 34104] loss: 1.0005 accuracy: 0.8137
[step: 34204] loss: 0.0601 accuracy: 0.9984
[step: 34304] loss: 0.0577 accuracy: 0.9975
==== epoch: 168, lr:0.0001 ====
[step: 34307] loss: 0.0576 accuracy: 0.9975
[Valid]: [step: 34307] loss: 1.0155 accuracy: 0.8125
[step: 34407] loss: 0.0544 accuracy: 0.9978
[step: 34507] loss: 0.0603 accuracy: 0.9978
==== epoch: 169, lr:0.0001 ====
[step: 34510] loss: 0.0609 accuracy: 0.9977
[Valid]: [step: 34510] loss: 1.0213 accuracy: 0.8113
[step: 34610] loss: 0.0548 accuracy: 0.9981
[step: 34710] loss: 0.0555 accuracy: 0.9980
==== epoch: 170, lr:0.0001 ====
[step: 34713] loss: 0.0553 accuracy: 0.9980
[Valid]: [step: 34713] loss: 1.0414 accuracy: 0.8119
[step: 34813] loss: 0.0560 accuracy: 0.9984
[step: 34913] loss: 0.0566 accuracy: 0.9986
==== epoch: 171, lr:0.0001 ====
[step: 34916] loss: 0.0566 accuracy: 0.9986
[Valid]: [step: 34916] loss: 1.0291 accuracy: 0.8081
[step: 35016] loss: 0.0518 accuracy: 0.9984
[step: 35116] loss: 0.0570 accuracy: 0.9978
==== epoch: 172, lr:0.0001 ====
[step: 35119] loss: 0.0569 accuracy: 0.9977
[Valid]: [step: 35119] loss: 1.0098 accuracy: 0.8119
[step: 35219] loss: 0.0526 accuracy: 0.9984
[step: 35319] loss: 0.0585 accuracy: 0.9978
==== epoch: 173, lr:0.0001 ====
[step: 35322] loss: 0.0585 accuracy: 0.9978
[Valid]: [step: 35322] loss: 1.0337 accuracy: 0.8019
[step: 35422] loss: 0.0508 accuracy: 0.9984
[step: 35522] loss: 0.0571 accuracy: 0.9978
==== epoch: 174, lr:0.0001 ====
[step: 35525] loss: 0.0575 accuracy: 0.9977
[Valid]: [step: 35525] loss: 1.0000 accuracy: 0.8106
[step: 35625] loss: 0.0548 accuracy: 0.9997
[step: 35725] loss: 0.0556 accuracy: 0.9986
==== epoch: 175, lr:0.0001 ====
[step: 35728] loss: 0.0558 accuracy: 0.9986
[Valid]: [step: 35728] loss: 1.0232 accuracy: 0.8063
[step: 35828] loss: 0.0563 accuracy: 0.9984
[step: 35928] loss: 0.0562 accuracy: 0.9983
==== epoch: 176, lr:0.0001 ====
[step: 35931] loss: 0.0560 accuracy: 0.9983
[Valid]: [step: 35931] loss: 1.0156 accuracy: 0.8081
[step: 36031] loss: 0.0512 accuracy: 0.9969
[step: 36131] loss: 0.0509 accuracy: 0.9977
==== epoch: 177, lr:0.0001 ====
[step: 36134] loss: 0.0507 accuracy: 0.9977
[Valid]: [step: 36134] loss: 1.0217 accuracy: 0.8113
[step: 36234] loss: 0.0542 accuracy: 0.9988
[step: 36334] loss: 0.0565 accuracy: 0.9983
==== epoch: 178, lr:0.0001 ====
[step: 36337] loss: 0.0564 accuracy: 0.9983
[Valid]: [step: 36337] loss: 1.0268 accuracy: 0.8100
[step: 36437] loss: 0.0571 accuracy: 0.9981
[step: 36537] loss: 0.0563 accuracy: 0.9981
==== epoch: 179, lr:0.0001 ====
[step: 36540] loss: 0.0565 accuracy: 0.9982
[Valid]: [step: 36540] loss: 1.0385 accuracy: 0.8063
[step: 36640] loss: 0.0529 accuracy: 0.9991
[step: 36740] loss: 0.0525 accuracy: 0.9986
==== epoch: 180, lr:0.0001 ====
[step: 36743] loss: 0.0524 accuracy: 0.9986
[Valid]: [step: 36743] loss: 1.0450 accuracy: 0.8094
[step: 36843] loss: 0.0499 accuracy: 0.9994
[step: 36943] loss: 0.0541 accuracy: 0.9983
==== epoch: 181, lr:0.0001 ====
[step: 36946] loss: 0.0541 accuracy: 0.9982
[Valid]: [step: 36946] loss: 1.0333 accuracy: 0.8125
[step: 37046] loss: 0.0521 accuracy: 0.9984
[step: 37146] loss: 0.0569 accuracy: 0.9980
==== epoch: 182, lr:0.0001 ====
[step: 37149] loss: 0.0571 accuracy: 0.9980
[Valid]: [step: 37149] loss: 1.0354 accuracy: 0.8081
[step: 37249] loss: 0.0495 accuracy: 0.9991
[step: 37349] loss: 0.0521 accuracy: 0.9991
==== epoch: 183, lr:0.0001 ====
[step: 37352] loss: 0.0527 accuracy: 0.9989
[Valid]: [step: 37352] loss: 1.0333 accuracy: 0.8081
[step: 37452] loss: 0.0556 accuracy: 0.9988
[step: 37552] loss: 0.0549 accuracy: 0.9981
==== epoch: 184, lr:0.0001 ====
[step: 37555] loss: 0.0548 accuracy: 0.9980
[Valid]: [step: 37555] loss: 1.0338 accuracy: 0.8137
[step: 37655] loss: 0.0552 accuracy: 0.9981
[step: 37755] loss: 0.0556 accuracy: 0.9977
==== epoch: 185, lr:0.0001 ====
[step: 37758] loss: 0.0567 accuracy: 0.9975
[Valid]: [step: 37758] loss: 1.0282 accuracy: 0.8056
[step: 37858] loss: 0.0542 accuracy: 0.9991
[step: 37958] loss: 0.0541 accuracy: 0.9988
==== epoch: 186, lr:0.0001 ====
[step: 37961] loss: 0.0545 accuracy: 0.9986
[Valid]: [step: 37961] loss: 1.0347 accuracy: 0.8044
[step: 38061] loss: 0.0553 accuracy: 0.9972
[step: 38161] loss: 0.0547 accuracy: 0.9977
==== epoch: 187, lr:0.0001 ====
[step: 38164] loss: 0.0560 accuracy: 0.9974
[Valid]: [step: 38164] loss: 1.0266 accuracy: 0.8081
[step: 38264] loss: 0.0552 accuracy: 0.9969
[step: 38364] loss: 0.0564 accuracy: 0.9973
==== epoch: 188, lr:0.0001 ====
[step: 38367] loss: 0.0563 accuracy: 0.9974
[Valid]: [step: 38367] loss: 1.0296 accuracy: 0.8075
[step: 38467] loss: 0.0583 accuracy: 0.9981
[step: 38567] loss: 0.0557 accuracy: 0.9981
==== epoch: 189, lr:0.0001 ====
[step: 38570] loss: 0.0554 accuracy: 0.9982
[Valid]: [step: 38570] loss: 1.0167 accuracy: 0.8025
[step: 38670] loss: 0.0518 accuracy: 0.9975
[step: 38770] loss: 0.0531 accuracy: 0.9981
==== epoch: 190, lr:0.0001 ====
[step: 38773] loss: 0.0533 accuracy: 0.9980
[Valid]: [step: 38773] loss: 1.0408 accuracy: 0.8094
[step: 38873] loss: 0.0502 accuracy: 0.9978
[step: 38973] loss: 0.0528 accuracy: 0.9975
==== epoch: 191, lr:0.0001 ====
[step: 38976] loss: 0.0530 accuracy: 0.9975
[Valid]: [step: 38976] loss: 1.0497 accuracy: 0.8019
[step: 39076] loss: 0.0536 accuracy: 0.9984
[step: 39176] loss: 0.0522 accuracy: 0.9986
==== epoch: 192, lr:0.0001 ====
[step: 39179] loss: 0.0521 accuracy: 0.9986
[Valid]: [step: 39179] loss: 1.0308 accuracy: 0.8144
[step: 39279] loss: 0.0516 accuracy: 0.9988
[step: 39379] loss: 0.0508 accuracy: 0.9983
==== epoch: 193, lr:0.0001 ====
[step: 39382] loss: 0.0506 accuracy: 0.9983
[Valid]: [step: 39382] loss: 1.0448 accuracy: 0.8137
[step: 39482] loss: 0.0493 accuracy: 0.9988
[step: 39582] loss: 0.0533 accuracy: 0.9984
==== epoch: 194, lr:0.0001 ====
[step: 39585] loss: 0.0532 accuracy: 0.9985
[Valid]: [step: 39585] loss: 1.0487 accuracy: 0.8075
[step: 39685] loss: 0.0477 accuracy: 0.9988
[step: 39785] loss: 0.0500 accuracy: 0.9984
==== epoch: 195, lr:0.0001 ====
[step: 39788] loss: 0.0502 accuracy: 0.9985
[Valid]: [step: 39788] loss: 1.0359 accuracy: 0.8100
[step: 39888] loss: 0.0457 accuracy: 0.9984
[step: 39988] loss: 0.0482 accuracy: 0.9983
==== epoch: 196, lr:0.0001 ====
[step: 39991] loss: 0.0480 accuracy: 0.9983
[Valid]: [step: 39991] loss: 1.0377 accuracy: 0.8137
[step: 40091] loss: 0.0505 accuracy: 0.9984
[step: 40191] loss: 0.0516 accuracy: 0.9977
==== epoch: 197, lr:0.0001 ====
[step: 40194] loss: 0.0513 accuracy: 0.9977
[Valid]: [step: 40194] loss: 1.0298 accuracy: 0.8063
[step: 40294] loss: 0.0520 accuracy: 0.9981
[step: 40394] loss: 0.0537 accuracy: 0.9980
==== epoch: 198, lr:0.0001 ====
[step: 40397] loss: 0.0535 accuracy: 0.9980
[Valid]: [step: 40397] loss: 1.0388 accuracy: 0.8150
[step: 40497] loss: 0.0461 accuracy: 0.9991
[step: 40597] loss: 0.0494 accuracy: 0.9981
==== epoch: 199, lr:0.0001 ====
[step: 40600] loss: 0.0492 accuracy: 0.9982
[Valid]: [step: 40600] loss: 1.0259 accuracy: 0.8106
